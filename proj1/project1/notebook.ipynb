{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Support Vector Machine Classification\n",
    "\n",
    "### NAME(S):\n",
    "### DATE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do?\n",
    "Using gradient descent, we will build a Support Vector Machine to find the optimal hyperplane that maximizes the margin between two toy data classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some use cases for SVMs?\n",
    "\n",
    "-Classification, regression (time series prediction, etc.), outlier detection, clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does an SVM compare to other ML algorithms?\n",
    "\n",
    "\n",
    "![alt text](images/img.png)\n",
    "Classifiers: (a) Logistic Regression, (b) SVM, and (c) Multi-Layer Perception (MLP)\n",
    "* As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers.\n",
    "* Other algorithms (Random forests, deep neural networks, etc.) require more data but almost always develop robust models.\n",
    "* The decision of which classifier to use depends on your dataset and the general complexity of the problem.\n",
    "* \"Premature optimization is the root of all evil (or at least most of it) in programming.\" - Donald Knuth, CS Professor (Turing award speech 1974)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "- Learning to use Scikit-learn's SVM function to classify images https://github.com/ksopyla/svm_mnist_digit_classification\n",
    "- Pulse classification, more useful dataset\n",
    "https://github.com/akasantony/pulse-classification-svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Support Vector Machine?\n",
    "\n",
    "It's a supervised machine learning algorithm that can be used for both classification and regression problems. But it's usually used for classification. Given two or more labeled data classes, it acts as a discriminative classifier, formally defined by an optimal hyperplane that separates all the classes. New examples mapped into that space can then be categorized based on which side of the gap they fall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Support Vectors?\n",
    "\n",
    "![alt text](images/SvmMargin2.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set; they help us build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is a hyperplane?\n",
    "\n",
    "![alt text](images/Hyperplanes+as+decision+surfaces.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with size $n âˆ’ 1$. By its nature, it separates the space in half.\n",
    "\n",
    "## Linear vs nonlinear classification?\n",
    "\n",
    "Sometimes our data is linearly separable. That means for N classes with M features. We can learn a mapping that is a linear combination. (like $y = mx + b$). Or even a multidimensional hyperplane ($y = x + z + b + q$). No matter how many dimensions/features a set of classes have, we can represent the mapping using a linear function.\n",
    "\n",
    "But sometimes it is not. Like if there was a quadratic mapping. Luckily for us, SVMs can efficiently perform a non-linear classification using what is called the kernel trick.\n",
    "\n",
    "![alt text](images/1_mCwnu5kXot6buL7jeIafqQ.png \"Logo Title Text 1\")\n",
    "\n",
    "More on this as a Bonus question comes at the end of notebook.\n",
    "\n",
    "All right, let's get to the building!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "In this assignment, you will implement a support vector machine (SVM) from scratch, and you will use your implementation for multiclass classification on the MNIST dataset.\n",
    "\n",
    "\n",
    "In `implementation.py` implement the SVM class. In the fit function, use `scipy.minimize` ([see documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)) to solve the constrained optimization problem:\n",
    "\n",
    "$\\begin{align*}\n",
    "& \\underset{a}{\\text{maximize}}& & \\sum_{i=1}^{n}a_{i} - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}(x_{i}\\cdot x_{j}) \\\\\n",
    "& \\text{subject to}& & a_{i} \\ge 0, i=1,\\ldots, n \\\\\n",
    "& & & \\sum_{i=1}^{n}a_{i}y_{i} = 0\n",
    "\\end{align*}$\n",
    "\n",
    "__Note__: An SVM is a convex optimization problem. Using  to solve the equation above will be computationally expensive given larger datasets. [CS 168 Convex Optimization](https://www.cs.tufts.edu/t/courses/description/spring2023/CS/168-01) is a course to take later if interested in optimization and the mathematics and intuition that drives it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from implementation import SVM, linear_kernel, nonlinear_kernel\n",
    "# from solution import SVM, linear_kernel, nonlinear_kernel\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Input data - of the form [Bias term, x_1 value, x_2 value]\n",
    "X = np.array([\n",
    "    [1, -2, 4,],\n",
    "    [1, 4, 1,],\n",
    "    [1, 1, 6,],\n",
    "    [1, 2, 4,],\n",
    "    [1, 6, 2,],\n",
    "])\n",
    "\n",
    "# Associated output labels - first 2 examples are labeled '-1' and last 3 are labeled '+1'\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "# Let's plot these examples on a 2D graph!\n",
    "# Plot the negative samples (the first 2)\n",
    "plt.scatter(X[:,1][y==-1], X[:,2][y==-1], s=120, marker='_', linewidths=2)\n",
    "# Plot the positive samples (the last 3)\n",
    "plt.scatter(X[:,1][y==1], X[:,2][y==1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Print a possible hyperplane, that is separating the two classes.\n",
    "# we'll two points and draw the line between them (naive guess)\n",
    "plt.plot([-2,6],[6,0.5])\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM basics\n",
    "SVM using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "result = SVC(kernel=\"linear\")\n",
    "result.fit(X, y.ravel())\n",
    "\n",
    "print(\"scikit-learn indices of support vectors:\", result.support_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement and test SVM to sklearn's version (20 points)\n",
    "Compare the indices of support vectors from scikit-lean with `implementation.py` using toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement SVM, along with linear_kernel\n",
    "\n",
    "result = SVC(kernel=\"linear\")\n",
    "result.fit(X, y)\n",
    "\n",
    "print(\"scikit-learn indices of support vectors:\", result.support_)\n",
    "\n",
    "svm = SVM(kernel=linear_kernel)\n",
    "svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"implementation.py indices of support vectors:\", \\\n",
    "      np.array(range(y.shape[0]))[svm.a>1e-8])\n",
    "\n",
    "if (result.support_ != np.array(range(y.shape[0]))[svm.a>1e-8]).all():\n",
    "    raise Exception(\"The calculation is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the weights assigned to the features from scikit-lean with `implementation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO - other sections were done for you, specify the variables to print, find the difference, and check it is within reasonable error from that of sklearn's version.\n",
    "# print(\"scikit-learn weights assigned to the features:\", VAR)\n",
    "# print(\"implementation.py weights assigned to the features:\", VAR)\n",
    "\n",
    "diff = np.nan #TODO\n",
    "if (diff > 1e-3).any():\n",
    "    raise Exception(\"The calculation is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the bias weight from scikit-lean with `implementation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"scikit-learn bias weight:\", result.intercept_)\n",
    "print(\"implementation.py bias weight:\", svm.b)\n",
    "\n",
    "diff = abs(result.intercept_ - svm.b)\n",
    "if (diff > 1e-3).all():\n",
    "    raise Exception(\"The calculation is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predictions from scikit-lean with `implementation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([\n",
    "    [4, 4, -1],\n",
    "    [1, 3, -1]\n",
    "    ])\n",
    "print(\"scikit-learn predictions:\", result.predict(X_test))\n",
    "print(\"implementation.py predictions:\", svm.predict(X_test))\n",
    "\n",
    "if (svm.predict(X_test) != result.predict(X_test)).all():\n",
    "    raise Exception(\"The calculation is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SKLearns SVM (*one-versus-the-rest*)\n",
    "\n",
    "You can load the data with `scipy.io.loadmat`, which will return a Python dictionary containing the test and train data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = loadmat('data/MNIST.mat')\n",
    "train_samples = mnist['train_samples']\n",
    "train_samples_labels = mnist['train_samples_labels']\n",
    "test_samples = mnist['test_samples']\n",
    "test_samples_labels = mnist['test_samples_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the MNIST dataset\n",
    "Explore the MNIST dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize samples of each class\n",
    "# TODO: Display counts of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *one-versus-the-rest* (15 Points) and analysis\n",
    "Using your implementation, compare multiclass classification performance of *one-versus-the-rest*\n",
    "\n",
    "**Create your own implementation of *one-versus-the-rest* and *one-versus-one*. Do not use sklearns multiclass SVM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO loop over classes training one_versus_the_rest()\n",
    "# TODO save all the prediction probability by predict_prob() for the following function\n",
    "# Hint: svm = SVC(kernel=\"linear\", probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_accuracy = np.nan #TODO\n",
    "test_accuracy = np.nan #TODO\n",
    "print(\"Train accuracy: {:.2f}\".format(100*train_accuracy))\n",
    "print(\"Test accuracy: {:.2f}\".format(100*test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $C>0$ controls the tradeoff between the size of the margin and the slack variable penalty. It is analogous to the inverse of a regularization coefficient. Include in your report a brief discussion of how you found an appropriate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Hint: Try using np.logspace for hyperparameter tuning\n",
    "# TODO: Find an appropriate value of C. \n",
    "train_accuracies = list()\n",
    "test_accuracies = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide details on how you found an appropriate value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot accuracies for train and test using logspace for x-axis (i.e., $C$ values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this graph tell us about the importance of our C value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: Analyze the plot above: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10 Points)\n",
    "In addition to calculating percent accuracy, generate multiclass [confusion matrices](https://en.wikipedia.org/wiki/confusion_matrix) as part of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = list()\n",
    "test_predictions = list()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (15 points)\n",
    "\n",
    "Now we will report our results and compare to other algorithms. Usually compare with a handful\n",
    "Logisitic regression\n",
    "\n",
    "**Create your own implementation of *one-versus-the-rest* and *one-versus-one*. Do not use sklearns multiclass Logistic Regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = list()\n",
    "test_predictions = list()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table comparing model accuracy on train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 9 graphs (one for each label) with two ROC curves (one for each model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (+5 points): Non-linear kernel\n",
    "## Intuition Behind Kernels\n",
    "The SVM classifier obtained by solving the convex Lagrange dual of the primal max-margin SVM formulation is as follows:\n",
    "\n",
    "$$ f(x) = \\sum_{i=1}^N\\alpha_i\\cdot y_i\\cdot\\mathrm{K}(x,x_i)+b,$$\n",
    "\n",
    "where $N$ is the number of support vectors.\n",
    "\n",
    "If you know the intuition behind a linear discriminant function, the non-parametric SVM classifier above is very easy to understand. Instead of imagining the original features of each data point, consider a transformation to a new feature space where the data point has $N$ features, one for each support vector. The value of the $i^{th}$ feature is equal to the value of the kernel between the $i^{th}$ support vector and the data point is classified. The original (possibly non-linear) SVM classifier is like any other linear discriminant in this space.\n",
    "\n",
    "Note that after the transformation, the original features of the data point are irrelevant. Its dot products with support vectors (special data points chosen by the SVM optimization algorithm) represent it only. One of my professors used a loose analogy while explaining this idea: A person has seen lakes, rivers, streams, fords, etc., but has never seen the sea. How would you explain to this person what a sea is? By relating the amount of water in an ocean to that found in a water body, the person already knows, etc.\n",
    "\n",
    "In some instances, like the RBF kernel, defining the transformed features in terms of the original features of a data point leads to an infinite-dimensional representation. Unfortunately, though this an awe-inspiring fact often mentioned while explaining how powerful SVMs are, it drops in only after repeated encounters with the idea ranging from introductory machine learning to statistical learning theory.\n",
    "\n",
    "\n",
    "## Intuition Behind Gaussian Kernels\n",
    "\n",
    "The Gaussian/RBF kernel is as follows:\n",
    "\n",
    "$$ \\mathrm{K}(x,y)=exp(âˆ’\\frac{||xâˆ’y||^2}{2\\sigma^2})$$\n",
    "\n",
    "\n",
    "Like any other kernel, we can understand the RBF kernel regarding feature transformation via the dot products given above. However, the intuition that helps best when analyzing the RBF kernel is that of the Gaussian distribution (as provided by [Akihiro Matsukawa](https://www.quora.com/profile/Akihiro-Matsukawa)).\n",
    "\n",
    "The Gaussian kernel computed with a support vector is an exponentially decaying function in the input feature space, the maximum value of which is attained at the support vector and which decays uniformly in all directions around the support vector, leading to hyper-spherical contours of the kernel function. The SVM classifier with the Gaussian kernel is simply a weighted linear combination of the kernel function computed between a data point and each support vector. The role of a support vector in the classification of a data point gets tempered with $\\alpha$, the global prediction usefulness of the support vector, and $\\mathrm{K}(x,y)$, the local influence of a support vector in prediction at a particular data point.\n",
    "\n",
    "In the 2D feature space, each support vector's kernel function's heat map decay away from the support vector and the resulting classifier (see the following figure).\n",
    "\n",
    "\n",
    "## Notion of Universal Kernels\n",
    "\n",
    "(This comes from learning theory, it could be more intuitive, but good to know.)\n",
    "\n",
    "Gaussian kernels are universal kernels, i.e., their use with appropriate regularization guarantees a globally optimal predictor, which minimizes a classifier's estimation and approximation errors. Here, we incur approximation error by limiting the space of classification models over which the search space. Estimation error refers to errors in estimating the model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition Behind Gaussian Kernels\n",
    "\n",
    "The Gaussian/RBF kernel is as follows:\n",
    "\n",
    "$$ \\mathrm{K}(x,y)=exp(âˆ’\\frac{||xâˆ’y||^2}{2\\sigma^2})$$\n",
    "\n",
    "\n",
    "Like any other kernel, we can understand the RBF kernel regarding feature transformation via the dot products given above. However, the intuition that helps best when analyzing the RBF kernel is that of the Gaussian distribution (as provided by [Akihiro Matsukawa](https://www.quora.com/profile/Akihiro-Matsukawa)).\n",
    "\n",
    "The Gaussian kernel computed with a support vector is an exponentially decaying function in the input feature space, the maximum value of which is attained at the support vector and which decays uniformly in all directions around the support vector, leading to hyper-spherical contours of the kernel function. The SVM classifier with the Gaussian kernel is simply a weighted linear combination of the kernel function computed between a data point and each support vector. The role of a support vector in the classification of a data point gets tempered with $\\alpha$, the global prediction usefulness of the support vector, and $\\mathrm{K}(x,y)$, the local influence of a support vector in prediction at a particular data point.\n",
    "\n",
    "In the 2D feature space, each support vector's kernel function's heat map decay away from the support vector and the resulting classifier (see the following figure).\n",
    "\n",
    "![gkernel in 2D](images/gkernel-2d.jpeg \" kernel function of each support vector\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notion of Universal Kernels\n",
    "(This comes from learning theory, it could be more intuitive, but good to know.)\n",
    "\n",
    "Gaussian kernels are universal kernels, i.e., their use with appropriate regularization guarantees a globally optimal predictor, which minimizes a classifier's estimation and approximation errors. Here, we incur approximation error by limiting the space of classification models over which the search space. Estimation error refers to errors in estimating the model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `nonlinear_kernel()` in `implementation.py`, use it, and compare with others (repeat above for SVM using non-linear kernel and do analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# (Bonus) TODO "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
